{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据建模  \n",
    "数据建模在机器学习中具有极其重要的地位。它是机器学习的核心组成部分，对于训练和评估模型、做出预测和优化决策都至关重要。\n",
    "\n",
    "模型训练和学习：在机器学习中，模型通过从数据中学习模式和关系来进行训练。模型的性能和准确性取决于所用数据的质量和数量。良好的数据建模可以提供高质量的训练数据，有助于构建更准确和可靠的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类数据建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习的一般框架：\n",
    "训练集 => 提取特征向量 => 结合一定的算法（分类器：比如决策树、KNN）=>得到结果\n",
    "\n",
    "支持向量机（support vector machines，SVM）是一种二分类模型，它将实例的特征向量映射为空间中的一些点，SVM 的目的就是想要画出一条线，以 “最好地” 区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。SVM 适合中小型数据样本、非线性、高维的分类问题。\n",
    "\n",
    "SVM 是有监督的学习模型，就是说我们需要先对数据打上标签，之后通过求解最大分类间隔来求解二分类问题，而对于多分类问题，可以组合多个 SVM 分类器来处理。\n",
    "### SVM 基本概念  \n",
    "将实例的特征向量（以二维为例）映射为空间中的一些点，如下图的实心点和空心点，它们属于不同的两类。SVM 的目的就是想要画出一条线，以“最好地”区分这两类点，以至如果以后有了新的点，这条线也能做出很好的分类。\n",
    "\n",
    "![image](https://github.com/X-lab2017/OpenTEA101/assets/115639837/f85c6a50-aca5-44d9-85e0-d1a6b4b57280)\n",
    "\n",
    "其实在工具 sklearn 中，已经封装了多种 SVM 模型，这里我们重点介绍下 SVC，该模型既可以训练线性可分的数据，也可以训练线性不可分数据。\n",
    "\n",
    "![image](https://github.com/X-lab2017/OpenTEA101/assets/115639837/36c6bc6f-7050-4588-b2a1-24faed758817)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GitHub机器人检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始之前，确保已经安装了Python，并且安装相应的两个库。可以通过 pip install pandas 和 pip install scikit-learn 来安装。\n",
    "完成以上步骤后，Pandas 和 Scikit-Learn 将安装在你的 Python 环境中，你可以在代码中导入它们并开始使用它们。例如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据探索\n",
    "\n",
    "我们先来加载数据，查看下数据情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  actor_id  type  public_repos  public_gists  followers  \\\n",
      "0           0       139  User           176             8       1118   \n",
      "1           1      1812  User            33             4         47   \n",
      "2           2      3416  User           104           132        367   \n",
      "3           3      3467  User            78            59        139   \n",
      "4           4      3579  User           210           101        188   \n",
      "\n",
      "   following  \n",
      "0          9  \n",
      "1          1  \n",
      "2          8  \n",
      "3         57  \n",
      "4         47  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('actor.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的字段含义：\n",
    "\n",
    "actor_id: GitHub用户的唯一标识符。  \n",
    "public_repos: 用户的公共仓库数量。  \n",
    "public_gists: 用户的公共代码片段数量。  \n",
    "followers: 关注该用户的人数。  \n",
    "following: 该用户关注的人数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要预测的标签就是字段 type，是一个二分类的问题。   \n",
    "除去 id 和 type 字段，还有4个字段，它们都是用户特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 划分数据集，80%用于训练，20%用于测试\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理和模型训练介绍\n",
    "\n",
    "在进行机器学习任务时，首要任务之一是准备和处理数据，以便将其用于训练和评估机器学习模型。这个过程包括选择适当的特征，将数据划分为训练集和测试集，进行数据标准化，并选择合适的模型来进行训练和预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 选择用于预测的特征列：\n",
    "\n",
    "首先，我们需要选择哪些特征（或属性）将用于预测任务。在这个示例中，我们选择了以下特征列：'public_repos'，'public_gists'，'followers'，和 'following'。这些特征将用于训练模型来预测一个用户的 'type'，这个类型可能代表了用户在某个社交媒体或平台上的活跃程度等信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 选择用于预测的特征列\n",
    "feature_columns = ['public_repos', 'public_gists', 'followers', 'following']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 训练集和测试集的输入特征：\n",
    "\n",
    "我们将数据分成两个部分：训练集和测试集。训练集将用于模型的训练，而测试集将用于评估模型的性能。我们需要从原始数据中提取输入特征并将其分配给相应的训练集（X_train）和测试集（X_test）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 训练集的输入特征\n",
    "X_train = train_df[feature_columns]\n",
    "\n",
    "# 5. 测试集的输入特征\n",
    "X_test = test_df[feature_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 训练集和测试集的标签：\n",
    "\n",
    "除了输入特征外，我们还需要确定目标标签，也就是我们要预测的内容。在这个例子中，我们的目标是用户的 'type'。我们将 'type' 列分配给训练集标签（y_train）和测试集标签（y_test）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 训练集的标签\n",
    "y_train = train_df['type']\n",
    "\n",
    "# 7. 测试集的标签\n",
    "y_test = test_df['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 数据标准化：\n",
    "\n",
    "在训练机器学习模型之前，通常需要对数据进行标准化处理，以确保不同特征的值具有相似的范围，以避免某些特征对模型的影响过大。我们使用标准化器（StandardScaler）来对输入特征进行标准化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 创建一个SVM分类器：\n",
    "\n",
    "在数据准备工作完成后，我们选择了一个支持向量机（SVM）分类器来进行模型训练。SVM是一种强大的监督学习算法，适用于分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9. 创建一个SVM分类器\n",
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 训练模型：\n",
    "\n",
    "接下来，我们使用训练集的输入特征（X_train_scaled）和标签（y_train）来训练SVM模型。模型将学习如何根据输入特征来预测用户的 'type'。\n",
    "\n",
    "7. 在测试集上进行预测：\n",
    "\n",
    "最后，我们使用训练好的模型来对测试集的输入特征（X_test_scaled）进行预测，以评估模型的性能和准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. 训练模型\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 11. 在测试集上进行预测\n",
    "y_pred = clf.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 评估模型性能\n",
    "\n",
    "现在，我们已经训练了一个支持向量机（SVM）模型，并使用测试集进行了预测。评估模型性能是确定模型对于实际数据的预测能力的关键步骤。在这里，我们将使用准确度（accuracy）作为性能指标来评估模型的表现。\n",
    "\n",
    "准确度（Accuracy）是一个用于分类问题的常见性能指标，它表示模型正确预测的样本占总样本数的比例。简而言之，它衡量了模型的整体预测精度。\n",
    "\n",
    "接下来的代码将计算模型的准确度并输出结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型的准确度: 0.9895697522816167\n"
     ]
    }
   ],
   "source": [
    "# 12. 评估模型性能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"模型的准确度:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Accuracy: 0.99\n",
      "F1 Score: 0.98\n",
      "Recall: 0.99\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Accuracy: 0.99\n",
      "F1 Score: 0.98\n",
      "Recall: 0.99\n",
      "\n",
      "Classifier: Random Forest\n",
      "Accuracy: 0.99\n",
      "F1 Score: 0.98\n",
      "Recall: 0.99\n",
      "\n",
      "Classifier: Naive Bayes\n",
      "Accuracy: 0.96\n",
      "F1 Score: 0.97\n",
      "Recall: 0.96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv('actor.csv')\n",
    "\n",
    "# 2. Split the dataset into 80% training and 20% testing\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Select the feature columns for prediction\n",
    "feature_columns = ['public_repos', 'public_gists', 'followers', 'following']\n",
    "\n",
    "# 4. Training set input features\n",
    "X_train = train_df[feature_columns]\n",
    "\n",
    "# 5. Testing set input features\n",
    "X_test = test_df[feature_columns]\n",
    "\n",
    "# 6. Training set labels\n",
    "y_train = train_df['type']\n",
    "\n",
    "# 7. Testing set labels\n",
    "y_test = test_df['type']\n",
    "\n",
    "# 8. Data Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a list of classifiers to compare\n",
    "classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('Naive Bayes', GaussianNB())\n",
    "]\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers:\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章节实践了SVM 分类的例子，希望你从这个过程中能够体会到整个项目的执行流程，包括数据加载、数据探索、SVM 训练和评估等环节。\n",
    "\n",
    "我们也能够看到，sklearn 已经为我们提供了大部分的实现，我们所需要做的就是理解业务（数据），找出最优的超参数，而把其他繁琐的数学运算先暂时放到一旁。我们需要在实战当中，熟悉流程，不断的训练自身的数据化思维和数据敏感度。\n",
    "\n",
    "总之，数据建模是机器学习的基石，影响着模型的质量、性能和实用性。在整个机器学习项目中，从数据收集、清洗、特征工程、模型训练到模型部署和维护，都需要关注和优化数据建模过程，以获得最佳的结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
