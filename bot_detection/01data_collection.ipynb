{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据收集\n",
    "\n",
    "在数据分析过程中，数据收集是至关重要的第一步。通常我们可以通过 Python 脚本收集、API 收集、数据生成这几个方法获得数据。\n",
    "\n",
    "知识点：Python 脚本收集、API 收集、数据生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 脚本收集\n",
    "\n",
    "脚本可以帮助我们自动化地从不同来源获取新闻、电影评论、股票、天气、商品价格和社交媒体等数据。使用Python来作为数据收集的脚本语言有多个优势，这些优势使得Python成为数据科学和数据工程领域的首选语言：\n",
    "\n",
    "1. **易学易用**： Python 被广泛认为是一门易学易用的编程语言，因此非常适合初学者。其简洁的语法和清晰的代码结构使得编写数据收集脚本相对容易，减少了学习曲线。\n",
    "\n",
    "2. **丰富的库和框架**： Python 拥有众多的数据科学和数据工程库，如 NumPy 、 Pandas 、 Matplotlib 、 Requests 等，这些库提供了各种数据操作、分析和可视化工具，可以大大简化数据收集和处理的任务。\n",
    "\n",
    "3. **强大的生态系统**： Python 生态系统拥有大量的第三方库和工具，例如 Beautiful Soup 和 Scrapy 用于网页抓取、 SQLAlchemy 用于数据库连接等等。这些工具可帮助用户轻松地处理各种数据源。\n",
    "\n",
    "4. **跨平台支持**： Python 是跨平台的编程语言，可以在 Windows 、 macOS 和 Linux 等操作系统上运行，这使得它适用于不同的开发环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备工作\n",
    "\n",
    "在开始之前，确保已经安装了Python，并且安装相应的两个库。可以通过 `pip install requests` 和 `pip install beautifulsoup4` 来安装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Requests库进行HTTP请求\n",
    "\n",
    "Python的requests库是一种流行的HTTP库，它允许用户轻松发送HTTP请求并获取网页内容。以下是一些示例用法：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 发送GET请求\n",
    "response = requests.get(\"https://baidu.com\")\n",
    "\n",
    "# 获取网页内容\n",
    "html_content = response.text\n",
    "\n",
    "# 打印网页内容\n",
    "print(html_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Beautiful Soup解析网页内容\n",
    "\n",
    "一旦获取了网页的HTML内容，通常需要使用解析库来提取所需的信息。Beautiful Soup是一个强大的库，用于解析HTML和XML文档，并提供了简便的方法来导航和搜索文档。以下是一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 使用Beautiful Soup解析HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser',from_encoding='utf-8')\n",
    "\n",
    "# 查找特定标签\n",
    "title = soup.title\n",
    "print(\"标题:\", title.text)\n",
    "\n",
    "# 查找所有链接\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    print(link.get('href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用CSS选择器或XPath进行元素选择\n",
    "\n",
    "在Beautiful Soup中，您可以使用CSS选择器或XPath来选择和提取特定的HTML元素。这允许您以更精确的方式定位所需的数据。以下是一些示例：\n",
    "\n",
    "使用CSS选择器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择所有带有class=\"article\"的元素\n",
    "articles = soup.select('.article')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用XPath：（这里需要安装 lxml）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用XPath选择所有<h2>元素\n",
    "# headings = soup.xpath('//h2')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "\n",
    "# 使用Beautiful Soup解析HTML并指定解析器为lxml\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# 使用XPath选择所有<h2>元素\n",
    "headings = soup.xpath('//h2')\n",
    "\n",
    "# 打印选择的元素\n",
    "for heading in headings:\n",
    "    print(heading.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理动态网页\n",
    "\n",
    "某些网页使用JavaScript来动态加载数据，这可能需要使用更高级的技术。您可以考虑使用Selenium等工具来模拟浏览器行为，并获取JavaScript生成的内容。\n",
    "\n",
    "如果需要使用Selenium，则需要在终端输入 `pip install Selenium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# 创建一个浏览器驱动程序\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# 打开网页\n",
    "driver.get(\"https://example.com\")\n",
    "\n",
    "# 等待一段时间，以确保页面加载完成\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# 获取动态加载的内容\n",
    "dynamic_content = driver.page_source\n",
    "\n",
    "# 关闭浏览器\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存数据\n",
    "\n",
    "可以用过Python将获得的数据存成html、CSV、JSON格式。保存至本地给其他任务使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### html格式存储\n",
    "\n",
    "- 首先发送HTTP GET请求获取网页内容，然后检查响应状态码以确保请求成功。\n",
    "- 接下来，我们指定要保存的文件路径和文件名（在这里是baidu_webpage.html），并使用open函数创建文件对象。\n",
    "- 然后，使用write方法将网页内容写入文件中。最后，我们在完成文件写入后关闭文件。\n",
    "\n",
    "请确保指定的文件路径和文件名是正确的，以及有适当的文件写入权限。以下示例将网页内容保存为HTML文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 发送HTTP GET请求获取网页内容\n",
    "url = \"http://baidu.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# 检查响应状态码，200表示成功\n",
    "if response.status_code == 200:\n",
    "    # 获取网页内容\n",
    "    html_content = response.text\n",
    "\n",
    "    # 指定要保存的文件路径和文件名\n",
    "    file_path = \"baidu_webpage.html\"\n",
    "\n",
    "    # 打开文件并写入网页内容\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"网页内容已保存到 {file_path}\")\n",
    "else:\n",
    "    print(\"请求失败\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON格式存储\n",
    "\n",
    "使用json模块来将数据写入JSON文件。json.dump函数用于将数据写入JSON文件，ensure_ascii=False参数用于处理非ASCII字符，indent参数用于指定缩进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# 发送HTTP GET请求获取数据\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# 检查响应状态码，200表示成功\n",
    "if response.status_code == 200:\n",
    "    # 获取数据（假设数据是JSON格式）\n",
    "    data = response.json()\n",
    "\n",
    "    # 指定要保存的JSON文件路径和文件名\n",
    "    json_file_path = \"data.json\"\n",
    "\n",
    "    # 将数据写入JSON文件\n",
    "    with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"数据已保存到 {json_file_path}\")\n",
    "else:\n",
    "    print(\"请求失败\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV格式存储\n",
    "\n",
    "和JSON格式存储类似，但不同的是CSV格式需要设置表头。使用csv模块来将数据写入CSV文件。根据数据的类型（列表或字典），我们使用不同的方法来写入数据。如果数据是列表，我们使用writerows方法写入多行数据。如果数据是字典，我们遍历字典中的键值对，并使用writerow方法写入每一行。请根据您的数据类型进行适当的调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    "# 发送HTTP GET请求获取数据\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# 检查响应状态码，200表示成功\n",
    "if response.status_code == 200:\n",
    "    # 获取数据（假设数据是列表或字典）\n",
    "    data = response.json()\n",
    "\n",
    "    # 指定要保存的CSV文件路径和文件名\n",
    "    csv_file_path = \"data.csv\"\n",
    "\n",
    "    # 将数据写入CSV文件\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "\n",
    "        # 如果数据是列表，可以直接写入\n",
    "        if isinstance(data, list):\n",
    "            csv_writer.writerows(data)\n",
    "        # 如果数据是字典，可以从字典中提取数据并写入\n",
    "        elif isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                csv_writer.writerow([key, value])\n",
    "\n",
    "    print(f\"数据已保存到 {csv_file_path}\")\n",
    "else:\n",
    "    print(\"请求失败\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网站说明\n",
    "\n",
    "`https://jsonplaceholder.typicode.com/posts` 是一个用于演示和测试的假数据 API（Application Programming Interface）。它是一个由 JSONPlaceholder 提供的免费的开发者工具，旨在模拟一个RESTful API，并提供一些虚构的数据，以便开发者可以在不访问真实数据的情况下测试和开发应用程序。\n",
    "\n",
    "该 API 提供了一些常见的资源，如帖子（posts）、评论（comments）、用户（users）等，可以用于测试和学习如何与 RESTful API 进行交互。开发者可以通过发送 HTTP 请求（例如，GET、POST、PUT、DELETE）来与该 API 进行通信，并获取、创建、更新和删除虚构的数据。\n",
    "\n",
    "这对于学习如何使用 HTTP 请求与 API 互动，以及如何处理返回的 JSON 数据非常有用。然而，请注意，这些数据都是虚构的，仅用于开发和测试目的，不包含真实的信息。如果您需要使用真实的数据，请查找适合您项目需求的实际 API。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 网页抓取的伦理和法律问题\n",
    "\n",
    "在进行网页抓取时，必须注意伦理和法律问题。确保遵守网站的使用政策，不要过度频繁地请求网页以避免对服务器造成负担。某些网站可能会禁止自动抓取，因此请仔细检查网站的robots.txt文件并尊重其规则。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样例: 获取电影评论数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功抓取并保存为CSV文件: titanic_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# 样例1 但是爬不到\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import csv\n",
    "\n",
    "\n",
    "def get_movie_reviews(movie_id,page_limit=10):\n",
    "\n",
    "    url_template=f'https://movie.douban.com/subject/{movie_id}/comments?start={{start}}&limit=20 sort=new_score&status=P'\n",
    "    # 设置请求头，模拟浏览器访问\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    reviews_data=[]\n",
    "\n",
    "    for i in range(page_limit):\n",
    "        url=url_template.format(start=i*20)\n",
    "        response=requests.get(url,headers=headers)\n",
    "        response.encoding='utf-8'\n",
    "\n",
    "        soup= BeautifulSoup(response.text,'lxml')\n",
    "\n",
    "        for item in soup.find_all('div',class_='comment_item'):\n",
    "            user_element=item.find('span',class_='comment-info').find('a')\n",
    "            user=user_element.text.strip()\n",
    "\n",
    "            rating_element=item.find_all('div',class_='rating')\n",
    "            rating=rating_element['title'] if rating_element else ''\n",
    "\n",
    "            comment_element= item.find('span',class_='short')\n",
    "            comment=comment_element.text.strip()\n",
    "\n",
    "            reviews_data.append({\n",
    "                'user':user,\n",
    "                'rating':rating,\n",
    "                'comment':comment,\n",
    "            })\n",
    "        \n",
    "    return reviews_data\n",
    "\n",
    "def save_to_csv(reviews_data,filename):\n",
    "    with open(filename,'w',newline='',encoding='utf-8') as csvfile:\n",
    "        fieldnames=['user','rating','comment']\n",
    "        writer=csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in reviews_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "movie_id='1292722' # 泰坦尼克号的电影ID\n",
    "reviews_data = get_movie_reviews(movie_id)\n",
    "save_to_csv(reviews_data,'titanic_reviews.csv')\n",
    "print('数据已成功抓取并保存为CSV文件: titanic_reviews.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功抓取并保存为CSV文件：titanic_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "# 样例2 成功爬取\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# 设置豆瓣电影页面的URL\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "url = 'https://movie.douban.com/subject/1292722/reviews'\n",
    "\n",
    "# 发送HTTP请求获取页面内容\n",
    "response=requests.get(url,headers=headers)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "else:\n",
    "    print(\"无法访问网页\")\n",
    "    exit()\n",
    "\n",
    "data = []\n",
    "\n",
    "# 查找评论元素\n",
    "reviews = soup.find_all('div', class_='review-item')\n",
    "\n",
    "# 遍历评论元素并提取信息\n",
    "for review in reviews:\n",
    "    reviewer = review.find('a', class_='name').text.strip()\n",
    "    rating = review.find('span', class_='main-title-rating')\n",
    "    if rating:\n",
    "        rating = rating.text.strip()\n",
    "    else:\n",
    "        rating = '未评分'\n",
    "    content = review.find('div', class_='short-content').text.strip()\n",
    "\n",
    "    # 将提取的信息添加到数据列表中\n",
    "    data.append([reviewer, rating, content])\n",
    "\n",
    "# 将数据保存到CSV文件\n",
    "with open('titanic_reviews.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['评论者', '评分', '评论内容'])  # 写入CSV文件头\n",
    "    csv_writer.writerows(data)  # 写入数据行\n",
    "\n",
    "print(\"数据已成功抓取并保存为CSV文件：titanic_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    "\n",
    "抓取新浪网最近一天的新闻数据，包括新闻主题、时间、摘要。抓取对应信息后，将信息存放在CSV文件中。\n",
    "\n",
    "新浪网的地址是 https://news.sina.com.cn/world"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
